{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--is_training IS_TRAINING]\n",
      "                             [--model_id MODEL_ID] [--model MODEL]\n",
      "                             [--plot_flag PLOT_FLAG] [--test_dir TEST_DIR]\n",
      "                             [--verbose VERBOSE] [--data DATA]\n",
      "                             [--root_path ROOT_PATH] [--data_path DATA_PATH]\n",
      "                             [--target TARGET] [--freq FREQ]\n",
      "                             [--checkpoints CHECKPOINTS]\n",
      "                             [--checkpoint_flag CHECKPOINT_FLAG]\n",
      "                             [--n_closest N_CLOSEST]\n",
      "                             [--all_stations ALL_STATIONS]\n",
      "                             [--data_step DATA_STEP]\n",
      "                             [--min_num_nodes MIN_NUM_NODES]\n",
      "                             [--features FEATURES] [--seq_len SEQ_LEN]\n",
      "                             [--label_len LABEL_LEN] [--pred_len PRED_LEN]\n",
      "                             [--enc_in ENC_IN] [--dec_in DEC_IN]\n",
      "                             [--c_out C_OUT] [--d_model D_MODEL]\n",
      "                             [--n_heads N_HEADS] [--e_layers E_LAYERS]\n",
      "                             [--d_layers D_LAYERS] [--gnn_layers GNN_LAYERS]\n",
      "                             [--d_ff D_FF] [--moving_avg MOVING_AVG]\n",
      "                             [--factor FACTOR] [--distil] [--dropout DROPOUT]\n",
      "                             [--embed EMBED] [--activation ACTIVATION]\n",
      "                             [--output_attention] [--win_len WIN_LEN]\n",
      "                             [--res_len RES_LEN] [--qk_ker QK_KER]\n",
      "                             [--v_conv V_CONV] [--sparse_flag SPARSE_FLAG]\n",
      "                             [--top_keys TOP_KEYS] [--kernel_size KERNEL_SIZE]\n",
      "                             [--train_strat_lstm TRAIN_STRAT_LSTM]\n",
      "                             [--norm_out NORM_OUT] [--num_decomp NUM_DECOMP]\n",
      "                             [--mlp_out MLP_OUT] [--num_workers NUM_WORKERS]\n",
      "                             [--itr ITR] [--train_epochs TRAIN_EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE] [--patience PATIENCE]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--lr_decay_rate LR_DECAY_RATE] [--des DES]\n",
      "                             [--loss LOSS] [--lradj LRADJ] [--use_gpu USE_GPU]\n",
      "                             [--gpu GPU] [--use_multi_gpu] [--devices DEVICES]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=c:\\Users\\natha\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-14376OV89cY2w8NMR.json could match --freq, --features, --factor\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def main():\n",
    "    fix_seed = 2022\n",
    "    random.seed(fix_seed)\n",
    "    torch.manual_seed(fix_seed)\n",
    "    np.random.seed(fix_seed)\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='FFTransformer, Transformer family, LSTM and MLP for Wind Forecasting')\n",
    "\n",
    "    # basic config\n",
    "    parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "    parser.add_argument('--model_id', type=str, required=False, default='test', help='model id for saving')\n",
    "    parser.add_argument('--model', type=str, required=False, default='FFTransformer',\n",
    "                        help='model name, options: [FFTransformer, Autoformer, Informer, Transformer, LogSparse, LSTM, MLP, persistence (and same with GraphXxxx)]')\n",
    "    parser.add_argument('--plot_flag', type=int, default=1, help='Whether to save loss plots or not')\n",
    "    parser.add_argument('--test_dir', type=str, default='', help='Base dir to save test results')\n",
    "    parser.add_argument('--verbose', type=int, default=1, help='Whether to print inter-epoch losses.')\n",
    "\n",
    "    # data loader\n",
    "    parser.add_argument('--data', type=str, required=False, default='WindGraph', help='dataset type, Wind or WindGraph')\n",
    "    parser.add_argument('--root_path', type=str, default='./dataset_example/WindData/dataset/', help='root path of the data file')\n",
    "    parser.add_argument('--data_path', type=str, default='wind_data.csv', help='data file') # wind_data\n",
    "    parser.add_argument('--target', type=str, default='44004', help='optional target station for non-graph models') #KVITEBJÃ˜RNFELTET\n",
    "    parser.add_argument('--freq', type=str, default='h', help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "    parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "    parser.add_argument('--checkpoint_flag', type=int, default=1, help='Whether to checkpoint or not')\n",
    "    parser.add_argument('--n_closest', type=int, default=None, help='number of closest nodes for graph connectivity, None --> complete graph')\n",
    "    parser.add_argument('--all_stations', type=int, default=0, help='Whether to use all stations or just target for non-spatial models.')\n",
    "    parser.add_argument('--data_step', type=int, default=1, help='Only use every nth point. Set data_step = 1 for full dataset.')\n",
    "    parser.add_argument('--min_num_nodes', type=int, default=2, help='Minimum number of nodes in a graph')\n",
    "\n",
    "    # forecasting task\n",
    "    parser.add_argument('--features', type=str, default='M', help='forecasting task, options:[M, S]; M:multivariate input, S:univariate input')\n",
    "    parser.add_argument('--seq_len', type=int, default=64, help='input sequence length')\n",
    "    parser.add_argument('--label_len', type=int, default=48, help='start token length. Note that Graph models only use label_len and pred_len')\n",
    "    parser.add_argument('--pred_len', type=int, default=6, help='prediction sequence length')\n",
    "    parser.add_argument('--enc_in', type=int, default=8, help='Number of encoder input features')\n",
    "    parser.add_argument('--dec_in', type=int, default=8, help='Number of decoder input features')\n",
    "    parser.add_argument('--c_out', type=int, default=1, help='output size, note that it is assumed that the target features are placed last')\n",
    "\n",
    "    # model define\n",
    "    parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "    parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "    parser.add_argument('--e_layers', type=int, default=2, help='number of encoder layers for non-spatial and number of LSTM or MLP layers for GraphLSTM and GraphMLP')\n",
    "    parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "    parser.add_argument('--gnn_layers', type=int, default=2, help='Number of sequential graph blocks in GNN')\n",
    "    parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "    parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average for Autoformer')\n",
    "    parser.add_argument('--factor', type=int, default=3, help='attn factor')\n",
    "    parser.add_argument('--distil', action='store_false', default=True, help='whether to use distilling in encoder, using this argument means not using distilling, not used for GNN models')\n",
    "    parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "    parser.add_argument('--embed', type=str, default='timeF', help='time features encoding, options:[timeF, fixed, learned]')\n",
    "    parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "    parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder', default=False)\n",
    "    parser.add_argument('--win_len', type=int, default=6, help='Local attention length for LogSparse Transformer')\n",
    "    parser.add_argument('--res_len', type=int, default=None, help='Restart attention length for LogSparse Transformer')\n",
    "    parser.add_argument('--qk_ker', type=int, default=4, help='Key/Query convolution kernel length for LogSparse Transformer')\n",
    "    parser.add_argument('--v_conv', type=int, default=0, help='Weather to apply ConvAttn for values (in addition to K/Q for LogSparseAttn')\n",
    "    parser.add_argument('--sparse_flag', type=int, default=1, help='Weather to apply logsparse mask for LogSparse Transformer')\n",
    "    parser.add_argument('--top_keys', type=int, default=0, help='Weather to find top keys instead of queries in Informer')\n",
    "    parser.add_argument('--kernel_size', type=int, default=3, help='Kernel size for the 1DConv value embedding')\n",
    "    parser.add_argument('--train_strat_lstm', type=str, default='recursive', help='The training strategy to use for the LSTM model. recursive or mixed_teacher_forcing')\n",
    "    parser.add_argument('--norm_out', type=int, default=1, help='Whether to apply laynorm to outputs of Enc or Dec in FFTransformer')\n",
    "    parser.add_argument('--num_decomp', type=int, default=4, help='Number of wavelet decompositions for FFTransformer')\n",
    "    parser.add_argument('--mlp_out', type=int, default=0, help='Whether to apply MLP to GNN outputs.')\n",
    "\n",
    "    # Optimization\n",
    "    parser.add_argument('--num_workers', type=int, default=0, help='data loader num workers')\n",
    "    parser.add_argument('--itr', type=int, default=1, help='experiments times')\n",
    "    parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "    parser.add_argument('--patience', type=int, default=5, help='early stopping patience')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001, help='optimizer learning rate')\n",
    "    parser.add_argument('--lr_decay_rate', type=float, default=0.8, help='Rate for which to decay lr with')\n",
    "    parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "    parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "    parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "\n",
    "    # GPU\n",
    "    parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "    parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "    # multi-gpu is not fully developed yet and still experimental for graph data.\n",
    "    parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "    parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multiple gpus')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.features == 'S':\n",
    "        assert (np.array([args.c_out, args.enc_in, args.dec_in]) == 1).all(), \"c_out, enc_in and dec_in should be 1 for univariate\"\n",
    "\n",
    "    args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "    if args.use_gpu and args.use_multi_gpu:\n",
    "        args.devices = args.devices.replace(' ', '')\n",
    "        device_ids = args.devices.split(',')\n",
    "        args.device_ids = [int(id_) for id_ in device_ids]\n",
    "        args.gpu = args.device_ids[0]\n",
    "\n",
    "    print('Args in experiment:')\n",
    "    print(args)\n",
    "\n",
    "    Exp = Exp_Main\n",
    "\n",
    "    if args.is_training:\n",
    "        for ii in range(args.itr):\n",
    "            # setting record of experiments\n",
    "            setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_{}'.format(\n",
    "                args.model_id,\n",
    "                args.model,\n",
    "                args.data,\n",
    "                args.features,\n",
    "                args.seq_len,\n",
    "                args.label_len,\n",
    "                args.pred_len,\n",
    "                ii)\n",
    "\n",
    "            exp = Exp(args)  # set experiments\n",
    "            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "            exp.train(setting)\n",
    "\n",
    "            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            exp.test(setting, base_dir=args.test_dir)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        ii = 0\n",
    "        setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_{}'.format(\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting, test=1)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
